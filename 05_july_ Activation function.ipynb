{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd60ee85-c906-4cab-ab7f-38ecd9cb44e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAn Activation Function decides whether a neuron should be activated or not. \\nThis means that it will decide whether the neuron's input to the network is important or not in the process\\nof prediction using simpler mathematical operations.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 1 :\n",
    "\"\"\"\n",
    "An Activation Function decides whether a neuron should be activated or not. \n",
    "This means that it will decide whether the neuron's input to the network is important or not in the process\n",
    "of prediction using simpler mathematical operations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dea3a0c-785d-46ba-a56e-906a8f5a7490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTypes of Activation Functions\\n1. Sigmoid Function\\n2. Hyperbolic Tangent Function (Tanh)\\n3. Softmax Function \\n4. Rectified Linear Unit (ReLU) Function \\n5. Leaky Relu\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No.2 :\n",
    "\"\"\"\n",
    "Types of Activation Functions\n",
    "1. Sigmoid Function\n",
    "2. Hyperbolic Tangent Function (Tanh)\n",
    "3. Softmax Function \n",
    "4. Rectified Linear Unit (ReLU) Function \n",
    "5. Leaky Relu\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5fb7e67-e972-4a01-9a4f-e94131214118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nActivation functions are a critical part of the design of a neural network.The choice of activation function\\nin the hidden layer will control how well the network model learns the training dataset. \\nThe choice of activation function in the output layer will define the type of predictions the model can make.\\n\\nActivation Functions are used to introduce non-linearity in the network. \\nA neural network will almost always have the same activation function in all hidden layers.This activation function\\nshould be differentiable so that the parameters of the network are learned in backpropagation. \\nReLU is the most commonly used activation function for hidden layers. \\nWhile selecting an activation function, you must consider the problems it might face: vanishing and exploding gradients. \\nRegarding the output layer, we must always consider the expected value range of the predictions. If it can be any \\nnumeric value (as in case of the regression problem) you can use the linear activation function or ReLU. \\nUse Softmax or Sigmoid function for the classification problems.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 3 :\n",
    "\"\"\"\n",
    "Activation functions are a critical part of the design of a neural network.The choice of activation function\n",
    "in the hidden layer will control how well the network model learns the training dataset. \n",
    "The choice of activation function in the output layer will define the type of predictions the model can make.\n",
    "\n",
    "Activation Functions are used to introduce non-linearity in the network. \n",
    "A neural network will almost always have the same activation function in all hidden layers.This activation function\n",
    "should be differentiable so that the parameters of the network are learned in backpropagation. \n",
    "ReLU is the most commonly used activation function for hidden layers. \n",
    "While selecting an activation function, you must consider the problems it might face: vanishing and exploding gradients. \n",
    "Regarding the output layer, we must always consider the expected value range of the predictions. If it can be any \n",
    "numeric value (as in case of the regression problem) you can use the linear activation function or ReLU. \n",
    "Use Softmax or Sigmoid function for the classification problems.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14135615-ba0e-40ec-b2db-4156aced4b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA sigmoidal function is a logistic function which purely refers to any function that retains the “S” shape, \\nfor example tanh(x). Where a traditional sigmoidal function exists between 0 and 1, tanh(x) follows a similar shape,\\nbut exists between 1 and -1. On its own, a sigmoidal function is also differentiable, we can easily find the slope of \\nthe sigmoid curve, at any given two points.\\n\\nAdvantages of Sigmoid Function: -\\n1. It provides Smooth gradient which helps us in preventing “jumps” in output values.\\n2. Output values bound between 0 and 1, normalizing the output of each neuron.\\n3. It provides clear predictions, i.e. very close to 1 or 0 which helps us to improve model performance.\\nDisadvantages of Sigmoid functions:\\n1. It is most prone to gradient vanishing problem.\\n2. Function output is not zero-centred.\\n3. Power operations are relatively time-consuming which increases model complexity.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 4 :\n",
    "\"\"\"\n",
    "A sigmoidal function is a logistic function which purely refers to any function that retains the “S” shape, \n",
    "for example tanh(x). Where a traditional sigmoidal function exists between 0 and 1, tanh(x) follows a similar shape,\n",
    "but exists between 1 and -1. On its own, a sigmoidal function is also differentiable, we can easily find the slope of \n",
    "the sigmoid curve, at any given two points.\n",
    "\n",
    "Advantages of Sigmoid Function: -\n",
    "1. It provides Smooth gradient which helps us in preventing “jumps” in output values.\n",
    "2. Output values bound between 0 and 1, normalizing the output of each neuron.\n",
    "3. It provides clear predictions, i.e. very close to 1 or 0 which helps us to improve model performance.\n",
    "Disadvantages of Sigmoid functions:\n",
    "1. It is most prone to gradient vanishing problem.\n",
    "2. Function output is not zero-centred.\n",
    "3. Power operations are relatively time-consuming which increases model complexity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfcdea1c-4b10-4317-81fd-bc51d9433b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe rectified linear activation function or ReLU for short is a piecewise linear function that will output the \\ninput directly if it is positive, otherwise, it will output zero.\\n\\nThe model trained with ReLU converged quickly and thus takes much less time when compared to models trained \\non the Sigmoid function. We can clearly see overfitting in the model trained with ReLU.\\nThis is due to the quick convergence. The model performance is significantly better when trained with ReLU.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 5 :\n",
    "\"\"\"\n",
    "The rectified linear activation function or ReLU for short is a piecewise linear function that will output the \n",
    "input directly if it is positive, otherwise, it will output zero.\n",
    "\n",
    "The model trained with ReLU converged quickly and thus takes much less time when compared to models trained \n",
    "on the Sigmoid function. We can clearly see overfitting in the model trained with ReLU.\n",
    "This is due to the quick convergence. The model performance is significantly better when trained with ReLU.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22d6e400-5294-4843-ac82-a42368cf328a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHistorical accident: we discovered ReLu in the early days before we knew about those tricks, so in the early days\\nReLu was the only choice that worked, and everyone had to use it. And now that everyone uses it, it is a safe choice\\nand people keep using it.\\n\\nEfficiency: ReLu is faster to compute than the sigmoid function, and its derivative is faster to compute. \\nThis makes a significant difference to training and inference time for neural networks: only a constant factor, \\nbut constants can matter.\\n\\nSimplicity: ReLu is simple.\\n\\nFragility: empirically, ReLu seems to be a bit more forgiving (in terms of the tricks needed to make the network \\ntrain successfully), whereas sigmoid is more fiddly (to train a deep network, you need more tricks,\\nand it's more fragile).\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 6 :\n",
    "\"\"\"\n",
    "Historical accident: we discovered ReLu in the early days before we knew about those tricks, so in the early days\n",
    "ReLu was the only choice that worked, and everyone had to use it. And now that everyone uses it, it is a safe choice\n",
    "and people keep using it.\n",
    "\n",
    "Efficiency: ReLu is faster to compute than the sigmoid function, and its derivative is faster to compute. \n",
    "This makes a significant difference to training and inference time for neural networks: only a constant factor, \n",
    "but constants can matter.\n",
    "\n",
    "Simplicity: ReLu is simple.\n",
    "\n",
    "Fragility: empirically, ReLu seems to be a bit more forgiving (in terms of the tricks needed to make the network \n",
    "train successfully), whereas sigmoid is more fiddly (to train a deep network, you need more tricks,\n",
    "and it's more fragile).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d14b3ad-4492-49de-aa10-c4787a844f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLeaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a\\nsmall slope for negative values instead of a flat slope.\\nThe slope coefficient is determined before training, i.e. it is not learnt during training\\n\\nBecause ReLU is known for vanishing gradients, since any values less than zero are mapped to zero. \\nThis is true regardless of the number of layers. LeakyReLU on the other hand, maps the values less than zeros\\nto a very small positive number.This prevents vanishing gradient from occurring.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 7 :\n",
    "\"\"\"\n",
    "Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a\n",
    "small slope for negative values instead of a flat slope.\n",
    "The slope coefficient is determined before training, i.e. it is not learnt during training\n",
    "\n",
    "Because ReLU is known for vanishing gradients, since any values less than zero are mapped to zero. \n",
    "This is true regardless of the number of layers. LeakyReLU on the other hand, maps the values less than zeros\n",
    "to a very small positive number.This prevents vanishing gradient from occurring.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bf44e83-d5a6-46fd-8f91-71a0a4c30b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe softmax function is used as the activation function in the output layer of neural network models that \\npredict a multinomial probability distribution. That is, softmax is used as the activation function for multi-class\\nclassification problems where class membership is required on more than two class labels.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 8 :\n",
    "\"\"\"\n",
    "The softmax function is used as the activation function in the output layer of neural network models that \n",
    "predict a multinomial probability distribution. That is, softmax is used as the activation function for multi-class\n",
    "classification problems where class membership is required on more than two class labels.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd9f8752-0776-43dc-b955-c0811a204b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe hyperbolic tangent activation function is also referred to simply as the Tanh (also “tanh” and “TanH“) function. \\nIt is very similar to the sigmoid activation function and even has the same S-shape.\\nThe function takes any real value as input and outputs values in the range -1 to 1.\\n\\nWe observe that the gradient of tanh is four times greater than the gradient of the sigmoid function. \\nThis means that using the tanh activation function results in higher values of gradient during training \\nand higher updates in the weights of the network.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 9 :\n",
    "\"\"\"\n",
    "The hyperbolic tangent activation function is also referred to simply as the Tanh (also “tanh” and “TanH“) function. \n",
    "It is very similar to the sigmoid activation function and even has the same S-shape.\n",
    "The function takes any real value as input and outputs values in the range -1 to 1.\n",
    "\n",
    "We observe that the gradient of tanh is four times greater than the gradient of the sigmoid function. \n",
    "This means that using the tanh activation function results in higher values of gradient during training \n",
    "and higher updates in the weights of the network.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
